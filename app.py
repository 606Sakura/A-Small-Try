# app.py
import os
import csv
import pickle
from pathlib import Path
from typing import List
from datetime import datetime

import streamlit as st
from sentence_transformers import SentenceTransformer
from sklearn.neighbors import NearestNeighbors
import numpy as np
import fitz  # PyMuPDF

# Optional imports
try:
    import openai
except Exception:
    openai = None

try:
    import tiktoken
except Exception:
    tiktoken = None

try:
    from transformers import pipeline
except Exception:
    pipeline = None

# ---------- é…ç½® ----------
INDEX_FILE = "index_store.pkl"
USAGE_LOG = "usage_log.csv"
MODEL_NAME = os.environ.get("SB_MODEL", "all-MiniLM-L6-v2")
MAX_CONTEXT_CHARS = 3000
st.set_page_config(page_title="RAG Chatbot MVP - Integrated", layout="wide")

# ä»¥ USD / per 1k tokens ä¸ºå•ä½ (input_price_per_1k, output_price_per_1k)
# è¯·åœ¨ç”Ÿäº§å‰æ ¸å¯¹ OpenAI å®˜æ–¹å®šä»·å¹¶æ›´æ–°æ­¤æ˜ å°„ã€‚
MODEL_PRICE_PER_1K = {
    "gpt-3.5-turbo": (0.00050, 0.00150),
    "gpt-4o": (0.00250, 0.01000),
    "gpt-4-0613": (0.03000, 0.06000),
    "gpt-4-32k": (0.06000, 0.12000),
    "gpt-4o-mini": (0.00015, 0.00060),
    "gpt-3.5-turbo-0613": (0.00150, 0.00200),
}

# ---------- å¸®åŠ©å‡½æ•° ----------
@st.cache_resource
def load_embedding_model(name=MODEL_NAME):
    return SentenceTransformer(name)

def read_txt(file_bytes) -> str:
    try:
        return file_bytes.decode("utf-8")
    except Exception:
        return file_bytes.decode("latin-1")

def read_pdf_bytes(bytes_data) -> str:
    doc = fitz.open(stream=bytes_data, filetype="pdf")
    text = ""
    for page in doc:
        text += page.get_text()
    return text

def chunk_text(text: str, chunk_size=300, overlap=50) -> List[str]:
    words = text.split()
    chunks = []
    i = 0
    while i < len(words):
        chunk = words[i:i+chunk_size]
        chunks.append(" ".join(chunk))
        i += chunk_size - overlap
    return [c for c in chunks if c.strip()]

def build_index(text_chunks: List[str], model):
    if len(text_chunks) == 0:
        raise ValueError("æ²¡æœ‰æ–‡æœ¬å—å¯ä»¥ç´¢å¼•")
    embs = model.encode(text_chunks, show_progress_bar=False, convert_to_numpy=True)
    nbrs = NearestNeighbors(n_neighbors=min(5, len(text_chunks)), metric="cosine").fit(embs)
    return {"nbrs": nbrs, "embs": embs, "chunks": text_chunks}

def save_index(obj, path=INDEX_FILE):
    with open(path, "wb") as f:
        pickle.dump(obj, f)

def load_index(path=INDEX_FILE):
    if Path(path).exists():
        with open(path, "rb") as f:
            return pickle.load(f)
    return None

def retrieve(query, model, nbrs_obj, top_k=3):
    if not nbrs_obj:
        return []
    q_emb = model.encode([query], convert_to_numpy=True)
    dists, idxs = nbrs_obj["nbrs"].kneighbors(q_emb, n_neighbors=min(top_k, len(nbrs_obj["chunks"])))
    results = []
    for dist, idx in zip(dists[0], idxs[0]):
        results.append((float(dist), nbrs_obj["chunks"][idx]))
    return results

def assemble_context(retrieved):
    parts = []
    total = 0
    for dist, txt in retrieved:
        if total + len(txt) > MAX_CONTEXT_CHARS:
            remaining = MAX_CONTEXT_CHARS - total
            if remaining > 0:
                parts.append(txt[:remaining])
            break
        parts.append(txt)
        total += len(txt)
    return "\n\n---\n\n".join(parts)

# ---------- Token counting / cost estimate ----------
def count_tokens_with_tiktoken(text: str, model_name: str = "gpt-3.5-turbo"):
    if tiktoken is None:
        return None
    try:
        enc = tiktoken.encoding_for_model(model_name)
    except Exception:
        try:
            enc = tiktoken.get_encoding("cl100k_base")
        except Exception:
            return None
    toks = enc.encode(text)
    return len(toks)

def estimate_tokens_and_cost(prompt_text: str, expected_completion_tokens: int, model_name: str):
    # Count prompt tokens
    prompt_tokens = None
    if tiktoken is not None:
        try:
            prompt_tokens = count_tokens_with_tiktoken(prompt_text, model_name)
        except Exception:
            prompt_tokens = None
    if prompt_tokens is None:
        prompt_tokens = max(1, int(len(prompt_text) / 4))

    total_tokens = prompt_tokens + expected_completion_tokens

    model_key = model_name.lower()
    cost_estimate = None
    if model_key in MODEL_PRICE_PER_1K:
        iprice, oprice = MODEL_PRICE_PER_1K[model_key]
        cost_estimate = (prompt_tokens / 1000.0) * iprice + (expected_completion_tokens / 1000.0) * oprice
    return {
        "prompt_tokens": prompt_tokens,
        "completion_tokens": expected_completion_tokens,
        "total_tokens": total_tokens,
        "estimated_cost_usd": cost_estimate,
    }

# ---------- OpenAI helper ----------
def call_openai_generate(prompt: str, system_prompt: str = None, model_name="gpt-3.5-turbo", max_tokens=256, temperature=0.2):
    if openai is None:
        return "OpenAI SDK æœªå®‰è£…ï¼ˆè¯·åœ¨ requirements ä¸­åŠ å…¥ openaiï¼‰", None

    api_key = os.environ.get("OPENAI_API_KEY")
    if not api_key:
        return "æœªæ£€æµ‹åˆ° OPENAI_API_KEY ç¯å¢ƒå˜é‡ï¼Œè¯·å…ˆè®¾ç½®ã€‚", None

    openai.api_key = api_key
    messages = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    messages.append({"role": "user", "content": prompt})

    try:
        resp = openai.ChatCompletion.create(
            model=model_name,
            messages=messages,
            max_tokens=max_tokens,
            temperature=temperature,
        )
        out = None
        try:
            out = resp.choices[0].message.get("content") if resp.choices else None
        except Exception:
            out = None
        if out is None:
            try:
                out = resp.choices[0].text if resp.choices else ""
            except Exception:
                out = ""
        usage = None
        try:
            usage = resp.get("usage") if isinstance(resp, dict) else getattr(resp, "usage", None)
        except Exception:
            usage = None
        return out.strip(), usage
    except Exception as e:
        return f"è°ƒç”¨ OpenAI å‡ºé”™ï¼š{e}", None

# ---------- Usage logging ----------
def append_usage_log(row: dict, path=USAGE_LOG):
    header = ["timestamp", "model", "prompt_tokens_est", "completion_tokens_est", "total_tokens_est", "estimated_cost_usd", "prompt_tokens_actual", "completion_tokens_actual", "total_tokens_actual"]
    write_header = not Path(path).exists()
    with open(path, "a", newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        if write_header:
            writer.writerow(header)
        writer.writerow([
            row.get("timestamp"),
            row.get("model"),
            row.get("prompt_tokens_est"),
            row.get("completion_tokens_est"),
            row.get("total_tokens_est"),
            row.get("estimated_cost_usd"),
            row.get("prompt_tokens_actual"),
            row.get("completion_tokens_actual"),
            row.get("total_tokens_actual"),
        ])

# ---------- UI ----------

st.title("ğŸ“š RAG Chatbot MVP â€” Integrated")
st.write("ä¸Šä¼ æ–‡ä»¶ â†’ æ„å»ºç´¢å¼• â†’ æé—® â†’ å¯é€‰æ‹©ç”Ÿæˆï¼ˆOpenAI / æœ¬åœ° HFï¼‰ã€‚é¦–æ¬¡åŠ è½½æ¨¡å‹ä¼šä» HuggingFace ä¸‹è½½ã€‚")

col_left, col_main = st.columns([1, 2])

with col_left:
    st.header("ä¸Šä¼  & ç´¢å¼•")
    uploaded = st.file_uploader("ä¸Šä¼  TXT æˆ– PDFï¼ˆå¯å¤šé€‰ï¼‰", type=["txt", "pdf"], accept_multiple_files=True)
    chunk_size = st.number_input("åˆ†å—å¤§å°ï¼ˆè¯ï¼‰", min_value=100, max_value=2000, value=300, step=50)
    overlap = st.number_input("åˆ†å—é‡å ï¼ˆè¯ï¼‰", min_value=0, max_value=1000, value=50, step=10)
    if st.button("æ„å»ºç´¢å¼•ï¼ˆæˆ–è¦†ç›–ï¼‰"):
        if not uploaded:
            st.warning("è¯·å…ˆä¸Šä¼ è‡³å°‘ä¸€ä¸ªæ–‡ä»¶")
        else:
            all_chunks = []
            for f in uploaded:
                try:
                    b = f.read()
                    if f.type == "text/plain" or f.name.lower().endswith(".txt"):
                        text = read_txt(b)
                    else:
                        text = read_pdf_bytes(b)
                except Exception as e:
                    st.error(f"è¯»å–æ–‡ä»¶ {f.name} å¤±è´¥ï¼š{e}")
                    text = ""
                if text:
                    chunks = chunk_text(text, chunk_size=chunk_size, overlap=overlap)
                    all_chunks.extend(chunks)
            if not all_chunks:
                st.error("æ²¡æœ‰å¯ç”¨æ–‡æœ¬ç‰‡æ®µ")
            else:
                with st.spinner("åŠ è½½ embedding æ¨¡å‹å¹¶å‘é‡åŒ–ï¼ˆç¬¬ä¸€æ¬¡ä¼šæ…¢ï¼‰..."):
                    emb_model = load_embedding_model()
                    try:
                        idx = build_index(all_chunks, emb_model)
                        st.session_state["index"] = idx
                        save_index(idx)
                        st.success(f"ç´¢å¼•æ„å»ºå®Œæˆï¼Œç‰‡æ®µæ•°é‡ï¼š{len(all_chunks)}ï¼Œå·²ä¿å­˜åˆ° {INDEX_FILE}")
                    except Exception as e:
                        st.error(f"ç´¢å¼•æ„å»ºå¤±è´¥ï¼š{e}")

    st.write("---")
    st.header("ç”Ÿæˆé€‰é¡¹")
    gen_method = st.selectbox("ç”Ÿæˆå™¨", options=["None", "OpenAI API", "Local HF (transformers)"])
    if gen_method == "OpenAI API":
        st.caption("éœ€è¦è®¾ç½®ç¯å¢ƒå˜é‡ OPENAI_API_KEYï¼›è°ƒç”¨å‰ä¼šæ˜¾ç¤º token ä¸è´¹ç”¨ä¼°ç®—ã€‚")
    if gen_method == "Local HF (transformers)":
        st.caption("æœ¬åœ°ç”Ÿæˆéœ€è¦ transformers + torchï¼›CPU ä¸Šå¯èƒ½å¾ˆæ…¢ã€‚")
    openai_model = st.text_input("OpenAI æ¨¡å‹åï¼ˆè‹¥ä½¿ç”¨ OpenAIï¼‰", value="gpt-3.5-turbo")
    gen_max_tokens = st.slider("ç”Ÿæˆæœ€å¤§ tokensï¼ˆcompletion æœ€å¤§é•¿åº¦ï¼‰", 64, 1024, 256)
    gen_temperature = st.slider("ç”Ÿæˆæ¸©åº¦ï¼ˆtemperatureï¼‰", 0.0, 1.0, 0.2, 0.05)

with col_main:
    st.header("é—®ç­” / èŠå¤©")
    if "index" not in st.session_state:
        st.info("è¯·å…ˆä¸Šä¼ å¹¶æ„å»ºç´¢å¼•ï¼Œæˆ–åŠ è½½ç¤ºä¾‹ã€‚")
    query = st.text_input("è¯·è¾“å…¥é—®é¢˜ï¼š", key="query_input")
    top_k = st.slider("æ£€ç´¢ç‰‡æ®µæ•° (top_k)", 1, 5, 3)

    if st.button("æŸ¥è¯¢") and query.strip():
        emb_model = load_embedding_model()
        idx = st.session_state.get("index", None)
        if not idx:
            st.error("æœªæ‰¾åˆ°ç´¢å¼•ï¼Œè¯·å…ˆæ„å»ºç´¢å¼•")
        else:
            results = retrieve(query, emb_model, idx, top_k=top_k)
            st.write("### æ£€ç´¢åˆ°çš„ç‰‡æ®µï¼ˆæŒ‰ç›¸ä¼¼åº¦ï¼‰")
            for i, (dist, txt) in enumerate(results):
                st.markdown(f"**ç‰‡æ®µ {i+1}ï¼ˆç›¸ä¼¼åº¦è·ç¦»={dist:.3f}ï¼‰**")
                st.write(txt[:1500] + ("..." if len(txt) > 1500 else ""))
            assembled = assemble_context(results)
            st.write("### åˆå¹¶åçš„ä¸Šä¸‹æ–‡ï¼ˆå·²æˆªæ–­ï¼‰")
            st.write(assembled[:4000] + ("..." if len(assembled) > 4000 else ""))

            prompt_template = (
                "ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†åº“é—®ç­”åŠ©æ‰‹ã€‚è¯·åŸºäºä¸‹é¢ä»çŸ¥è¯†åº“æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ï¼Œ"
                "ä»¥åŠç”¨æˆ·çš„é—®é¢˜ï¼Œç»™å‡ºç®€æ´ã€å‡†ç¡®ã€å¯å¼•ç”¨æ¥æºçš„å›ç­”ã€‚\n\n"
                "ä¸Šä¸‹æ–‡:\n{context}\n\nç”¨æˆ·é—®é¢˜:\n{question}\n\nå›ç­”:"
            )
            prompt = prompt_template.format(context=assembled, question=query)

            if gen_method == "OpenAI API":

                est = estimate_tokens_and_cost(prompt, expected_completion_tokens=gen_max_tokens, model_name=openai_model)
                st.write("**ä¼°ç®—ï¼ˆä»…ä¾›å‚è€ƒï¼‰**")
                st.write(f"- prompt tokens (ä¼°è®¡): {est['prompt_tokens']}")
                st.write(f"- completion tokens (è®¾å®šä¸Šé™): {est['completion_tokens']}")
                st.write(f"- total tokens (ä¼°è®¡): {est['total_tokens']}")
                if est["estimated_cost_usd"] is not None:
                    st.write(f"- ä¼°ç®—è´¹ç”¨ (USD): ${est['estimated_cost_usd']:.6f}")
                else:
                    st.write("- ä¼°ç®—è´¹ç”¨: æ¨¡å‹ä»·æ ¼æœªé…ç½®ï¼Œæ— æ³•ä¼°ç®—ï¼ˆè¯·åœ¨ä»£ç çš„ MODEL_PRICE_PER_1K æ·»åŠ æ¡ç›®ï¼‰")

                if st.button("Proceed with OpenAI call (will incur cost)"):
                    with st.spinner("è°ƒç”¨ OpenAI ç”Ÿæˆä¸­..."):
                        out, usage = call_openai_generate(prompt, system_prompt=None, model_name=openai_model, max_tokens=gen_max_tokens, temperature=gen_temperature)
                        st.write("### ç”Ÿæˆå›ç­”")
                        st.write(out)
                        # å†™å…¥ usage logï¼ˆè‹¥æœ‰ï¼‰
                        row = {
                            "timestamp": datetime.utcnow().isoformat(),
                            "model": openai_model,
                            "prompt_tokens_est": est['prompt_tokens'],
                            "completion_tokens_est": est['completion_tokens'],
                            "total_tokens_est": est['total_tokens'],
                            "estimated_cost_usd": est['estimated_cost_usd'],
                            "prompt_tokens_actual": None,
                            "completion_tokens_actual": None,
                            "total_tokens_actual": None,
                        }
                        if usage:
                            try:
                                pt = usage.get('prompt_tokens') or usage.get('input_tokens')
                                ct = usage.get('completion_tokens') or usage.get('output_tokens')
                                tt = usage.get('total_tokens')
                                row['prompt_tokens_actual'] = pt
                                row['completion_tokens_actual'] = ct
                                row['total_tokens_actual'] = tt
                            except Exception:
                                pass
                        try:
                            append_usage_log(row)
                            st.success('å·²å°† usage ä¿å­˜åˆ° ' + USAGE_LOG)
                        except Exception as e:
                            st.error('ä¿å­˜ usage å¤±è´¥ï¼š' + str(e))

            elif gen_method == "Local HF (transformers)":
                st.info("æœ¬åœ°ç”Ÿæˆï¼šå°† prompt ä¼ å…¥ transformers pipelineï¼ˆè‹¥å·²å®‰è£…ï¼‰ã€‚")
                if st.button("Local generate"):
                    if pipeline is None:
                        st.error("transformers æœªå®‰è£…æˆ–ä¸å¯ç”¨ï¼Œè¯·å®‰è£… transformers + torchã€‚")
                    else:
                        with st.spinner("æœ¬åœ°ç”Ÿæˆä¸­ï¼ˆCPU å¯èƒ½æ…¢ï¼‰..."):
                            try:
                                gen = pipeline("text2text-generation", model="google/flan-t5-small")
                                out = gen(prompt, max_length=gen_max_tokens, do_sample=False)
                                st.write("### æœ¬åœ°ç”Ÿæˆç»“æœ")
                                st.write(out[0].get("generated_text", str(out)))
                            except Exception as e:
                                st.error(f"æœ¬åœ°ç”Ÿæˆå¤±è´¥ï¼š{e}")
            else:
                st.info("ç”Ÿæˆå…³é—­ï¼šä»…è¿”å›æ£€ç´¢ç‰‡æ®µã€‚")

st.caption("Token counting via tiktoken (preferred). If tiktoken absent, a heuristic (1 token â‰ˆ 4 chars) is used.")
